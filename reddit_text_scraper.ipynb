{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de90dce-e2b6-4282-9a9c-22ede7d36ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://github.com/parth647/reddit_scraping_using_praw/blob/master/python_reddit_scrapy.py\n",
    "# Code from https://github.com/D3vd/Reddit_Image_Scraper/blob/master/Reddit_image_scraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1db14-156b-40eb-b37b-8de8a4234294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import praw\n",
    "from pmaw import PushshiftAPI #to allow to mine more than 1000 records\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from prawcore.exceptions import ResponseException\n",
    "\n",
    "from reddit_image_scrapper import get_img_subreddit, delete_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf536bb-ba8f-4c12-89f8-f41104b8d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_info():\n",
    "  config = configparser.ConfigParser()\n",
    "  config.read(\"config.ini\")\n",
    "  id = config[\"ALPHA\"][\"client_id\"]\n",
    "  secret = config[\"ALPHA\"][\"client_secret\"]\n",
    "\n",
    "  return id, secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e3c96-f3bb-4580-81b9-1b5ad3b99f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessing the reddit api\n",
    "\n",
    "clien_id, client_secret = get_client_info()\n",
    "reddit = praw.Reddit(client_id=clien_id,#my client id\n",
    "                     client_secret=client_secret,  #your client secret\n",
    "                     user_agent='retrieve kb data', #user agent name\n",
    "                     username = '',     # your reddit username\n",
    "                     password = '')     # your reddit password\n",
    "\n",
    "sub = ['MechanicalKeyboards']  # make a list of subreddits you want to scrape the data from\n",
    "# Alternate sub to mine: 'MechanicalKeyboards'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf73bcc-106e-4b8b-afa8-a9f85072ecd1",
   "metadata": {},
   "source": [
    "## To scrape stickied comments and pictures of the reddit post\n",
    "main file -> json with the following format\n",
    "{'sticky_comment': raw_text, 'title': raw_text, 'img_path': [path_to_saved_images]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e775963-5d35-41ce-ac4a-2179135cc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraped_dict = []\n",
    "\n",
    "delete_img_list() # Delete image list to get clean slate\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "api = PushshiftAPI(praw=reddit)\n",
    "\n",
    "try:\n",
    "  #for i, submission in enumerate(reddit.subreddit('MechGroupBuys').new(limit=2000)):    \n",
    "  for i, submission in enumerate(api.search_submissions(subreddit='MechGroupBuys', limit=3000)):    \n",
    "    print(i, submission.title)\n",
    "    title_pattern = '\\[(?:GB|Pre-order|Preorder|In-stock)\\](.*)/' #Bring text eda code to help save files\n",
    "    try:\n",
    "      title = re.findall(title_pattern, submission.title.replace('\\n', ''))[0].replace('/', '').strip()\n",
    "    except:\n",
    "      title = submission.title.replace('[GB]', '').strip()\n",
    "      title = title[:title.find('//')]\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join('data', f'scrapped_data_{title}.json')): continue\n",
    "    print(submission.title)\n",
    "    for top_level_comment in submission.comments:\n",
    "      if top_level_comment.stickied:        \n",
    "\n",
    "        # If the key is to extract the price, try to parse all rows into text file then manipulate from there\n",
    "        sticky_comment = top_level_comment.body\n",
    "\n",
    "    list_file_loc = get_img_subreddit(submission, num_img_limit=50, sleep_interval=1)\n",
    "    scraped_dict = {'title': submission.title, 'sticky_comment': sticky_comment, 'img_paths': list_file_loc}\n",
    "\n",
    "    with open(os.path.join('data', f'scrapped_data_{title}.json'), 'w') as json_output:\n",
    "      json.dump(scraped_dict, json_output, indent=2)\n",
    "\n",
    "    #list_scraped_dict.append(scraped_dict)\n",
    "      \n",
    "  #with open(\"scrapped_data.json\", \"w\") as json_output:\n",
    "   #json.dump(list_scraped_dict, json_output, indent=2)\n",
    "  \n",
    "          \n",
    "except ResponseException as e:\n",
    "  pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
